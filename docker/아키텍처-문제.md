도커가 운영체제 수준에서 격리된 공간을 제공한다는 점은 맞지만, 도커와 가상 머신(VM) 사이에는 아주 중요한 차이점이 하나 있습니다. 바로 **호스트의 커널(Kernel)을 공유한다**는 점입니다. 🐧

이 근본적인 차이 때문에 하드웨어의 설계 방식인 **아키텍처(CPU Architecture)** 문제가 발생하게 됩니다. 이해를 돕기 위해 가상 머신과 도커 컨테이너가 하드웨어를 어떻게 사용하는지 비교해 보겠습니다.

| 구분 | 가상 머신 (VM) 🖥️ | 도커 컨테이너 (Container) 📦 |
| --- | --- | --- |
| **핵심 기술** | 하드웨어 가상화 (Hypervisor) | 프로세스 격리 (Namespaces, Cgroups) |
| **커널** | 게스트 OS마다 독립적인 커널 존재 | **호스트 OS의 커널을 공유** |
| **CPU 아키텍처** | 에뮬레이션을 통해 다른 아키텍처 실행 가능 | **호스트 CPU의 명령어를 직접 실행** |

### 왜 아키텍처가 다르면 실행되지 않을까요?

이미지는 실행 파일과 라이브러리들의 집합입니다. M1 맥북(ARM64)에서 이미지를 빌드하면, 그 안의 실행 파일들은 **ARM CPU가 이해할 수 있는 명령어**로 작성됩니다.

하지만 이 이미지를 x86_64(Intel/AMD) 환경인 우분투에서 실행하려고 하면, 우분투 CPU는 ARM 명령어를 알아듣지 못해 오류가 발생하게 됩니다. 도커는 OS 환경은 격리해주지만, CPU가 사용하는 언어(명령어 세트)까지 통번역해주는 도구는 아니기 때문입니다.

가상 머신(VM)은 **'하드웨어'**를 흉내 내는 방식이고, 도커는 **'운영체제의 프로세스'**를 격리하는 방식이라는 점이 가장 큰 차이입니다.

가상 머신 (VM): 하이퍼바이저(Hypervisor)라는 소프트웨어가 CPU, 메모리, 네트워크 카드 등 가상 하드웨어를 통째로 만들어냅니다. 이 과정에서 다른 아키텍처의 CPU를 흉내 내는 에뮬레이션(Emulation) 기술을 사용하면, ARM 하드웨어 위에서도 x86용 커널과 OS를 올릴 수 있게 됩니다.

도커 (Docker): 하드웨어를 가상화하지 않습니다. 대신 호스트 OS의 커널(Kernel)을 그대로 공유하면서, 프로세스가 사용하는 자원만 논리적으로 격리합니다. 따라서 컨테이너 안의 실행 파일은 호스트 CPU가 직접 이해할 수 있는 **명령어 세트(ISA)**로 짜여 있어야만 합니다.

## 도커는 왜 커널을 가상화하지 않고 공유하는가?

도커가 커널을 공유하기로 선택한 가장 큰 이유는 **효율성**입니다. 가상 머신(VM)처럼 가상 하드웨어 위에 OS를 통째로 올리는 것은 너무 무겁고 느리기 때문에, 도커는 이미 실행 중인 호스트 OS의 핵심 기능을 빌려 쓰면서 프로세스가 사용하는 공간만 논리적으로 나누는 방식을 택했습니다.

### 커널 공유의 이점과 한계

| 구분 | 이점 (Benefits) 🚀 | 한계 (Limitations) ⚠️ |
| --- | --- | --- |
| **속도** | OS 부팅 과정이 생략되어 수 초 내에 실행됨 | 호스트 커널의 버전이나 설정에 영향을 받음 |
| **자원** | 하드웨어 가상화 오버헤드가 없어 자원 낭비가 적음 | CPU 아키텍처가 다르면 실행이 불가능함 |
| **밀도** | 하나의 호스트에 수백 개의 컨테이너를 올릴 수 있음 | 한 컨테이너의 커널 취약점이 호스트 전체로 확산 가능 |

도커는 커널을 공유하는 대신 **격리(Isolation)**를 통해 독립된 환경처럼 보이게 만듭니다. 이 메커니즘을 더 깊이 이해하기 위해 다음 중 어떤 내용을 먼저 살펴볼까요?

## 멀티 아키텍처 빌드

`docker buildx`가 내 컴퓨터와 다른 아키텍처의 이미지를 빌드할 수 있는 비결은 바로 **에뮬레이션(Emulation)** 기술에 있습니다. 🏗️

도커 자체는 커널을 공유하지만, `buildx`는 그 위에서 **QEMU**라는 오픈소스 에뮬레이터를 활용하여 마치 다른 아키텍처인 것처럼 속여서 작업을 수행합니다.

### 멀티 아키텍처 빌드의 핵심 요소

`docker buildx`가 작동하는 구조를 크게 세 가지 요소로 나누어 볼 수 있습니다.

| 요소 | 역할 |
| --- | --- |
| **BuildKit** 🛠️ | 도커의 차세대 빌드 엔진으로, 여러 플랫폼을 동시에 빌드하는 기능을 제공합니다. |
| **QEMU** 🔄 | 다른 아키텍처의 CPU 명령어를 현재 CPU가 이해할 수 있는 명령어로 실시간 번역해주는 에뮬레이터입니다. |
| **binfmt_misc** 🚦 | 리눅스 커널의 기능으로, 실행 파일의 형식을 확인하여 적절한 에뮬레이터(QEMU)로 연결해줍니다. |

### 어떻게 빌드가 진행될까요?

예를 들어, **x86(인텔/AMD)** 컴퓨터에서 **ARM(M1/M2 맥북용)** 이미지를 빌드한다고 가정해 보겠습니다.

1. `docker buildx` 명령을 내리면 **BuildKit**이 빌드를 시작합니다.
2. 빌드 과정 중 ARM용 명령어가 나오면, 리눅스 커널은 이를 감지하고 **QEMU**에게 전달합니다.
3. **QEMU**는 이 ARM 명령어를 x86 명령어로 번역하여 실제 CPU가 처리하게 합니다.
4. 결과적으로 ARM 아키텍처에서 실행될 수 있는 바이너리들이 포함된 이미지가 완성됩니다.

이 기술 덕분에 개발자는 하나의 컴퓨터에서 여러 환경(서버용, 모바일용, 임베디드용 등)에 맞는 이미지를 한 번에 만들어낼 수 있습니다. 🌍

여기서 한 가지 생각해 볼 점이 있습니다. CPU 명령어를 하나하나 실시간으로 '번역'하면서 빌드를 진행한다면, 원래 자기 아키텍처에서 빌드할 때와 비교해서 어떤 차이가 생길까요? 유저님의 생각이 궁금합니다.

1. **빌드 속도** ⏳: 번역 과정이 추가되므로 속도가 어떻게 변할까요?
2. **호환성** ✅: 에뮬레이션으로 만든 이미지가 실제 하드웨어에서 완벽하게 돌아갈까요?
3. **리소스 사용량** 🔋: CPU나 메모리를 얼마나 더 사용할까요?
